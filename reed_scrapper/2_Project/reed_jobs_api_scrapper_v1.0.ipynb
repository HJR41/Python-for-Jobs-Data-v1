{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import multiple librarys for Data request and cleanup\n",
    "\n",
    "import requests # this library handles http requests... for eg API GET requests.\n",
    "import os # enables me to retrieve api key from another file\n",
    "from dotenv import load_dotenv # allows me to access env file with api key\n",
    "import pandas as pd # for all things pandas\n",
    "import pgeocode\n",
    " # no longer required from geopy.geocoders import Nominatim # to clean up location data\n",
    "import json # to parse json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv(\"store.env\") # loads data from env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reed.co.uk/api/1.0/search?keywords=data&locationName=birmingham&employerId=&distanceFromLocation=40&resultsToTake=100&resultsToSkip=0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "versionnumber = \"1.0\"\n",
    "keywords = \"data\"\n",
    "locationName = \"birmingham\" # location search criterea\n",
    "employerId = \"\"\n",
    "distanceinmiles = \"40\"\n",
    "resultsToTake = 100\n",
    "resultsToSkip = 0\n",
    "reed_api_key = os.getenv(\"reed_api_key\") #retrieves api_key from .env file. This means i don't need to hard code my API key into script for improved security.\n",
    "today = datetime.today().strftime('%d.%m.%Y') # used in filename when exporting to excel\n",
    "url = f\"https://www.reed.co.uk/api/{versionnumber}/search?keywords={keywords}&locationName={locationName}&employerId={employerId}&distanceFromLocation={distanceinmiles}&resultsToTake={resultsToTake}&resultsToSkip={resultsToSkip}\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Success 200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, auth=(reed_api_key, ''))\n",
    "json_for_df = [] # initialise list to extend reed_data to.\n",
    "if response.status_code == 200:\n",
    "    print(f\"Response Success {response.status_code}\") \n",
    "    reed_data = response.json() # get json and set it to reed data. Add reed data to json_for_df below\n",
    "    json_for_df.extend(reed_data[\"results\"])\n",
    "    resultsToSkip = resultsToSkip + resultsToTake # increment results to skip so i don't append the same data in the loop below\n",
    "    total_results = (reed_data[\"totalResults\"])\n",
    "else:\n",
    "    raise Exception (f\"Unsuccesfull API request: {response.status_code}\") # stops execution of code and exits with error message.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages_minus_1 = ((total_results + resultsToTake - 1) // resultsToTake) - 1  # this gives the total number of pages minues 1 (because i already have the first API page) to retrieve data from.\n",
    "\n",
    "for num_page in range(total_pages_minus_1):  # ie from 0 to the total number of pages\n",
    "    print(\"-----\")\n",
    "    url = f\"https://www.reed.co.uk/api/{versionnumber}/search?keywords={keywords}&locationName={locationName}&employerId={employerId}&distanceFromLocation={distanceinmiles}&resultsToTake={resultsToTake}&resultsToSkip={resultsToSkip}\"\n",
    "    response = requests.get(url, auth=(reed_api_key, ''))\n",
    "    reed_data = response.json()\n",
    "    json_for_df.extend(reed_data[\"results\"])\n",
    "    print(\"requesting:\", url)\n",
    "    resultsToSkip = resultsToSkip + resultsToTake # increment results to skip by 100 for every page\n",
    "    if resultsToSkip >= 9900:\n",
    "        print(\"API request reached limit. exiting loop\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(json_for_df)\n",
    "# Data clean-up:\n",
    "df.fillna(np.nan, inplace=True) # this fills empty values with NaN\n",
    "df['date'] = pd.to_datetime(df['date'], format=\"%d/%m/%Y\") # turn the date into an actual date-time date\n",
    "\n",
    "df = df.drop_duplicates(subset='jobId')\n",
    "\n",
    "df_rows = len(df)\n",
    "\n",
    "print(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response sucess 200\n"
     ]
    }
   ],
   "source": [
    "# retrieve UK postcode data from pgeocode API and save in a df. ill do a merge between main df & geo_df to get location data\n",
    "country = \"GB\"\n",
    "\n",
    "download_url = f\"https://symerio.github.io/postal-codes-data/data/geonames/{country}.txt\"\n",
    "\n",
    "column_names = [\n",
    "    \"countryCode\",\n",
    "    \"outwardCode\", \n",
    "    \"nonPostcodeValues\", \n",
    "    \"countryName\", \n",
    "    \"countryAbv\", \n",
    "    \"countyName\", \n",
    "    \"admin_1\", \n",
    "    \"districtName\", \n",
    "    \"onsCode\", \n",
    "    \"latitude\", \n",
    "    \"longitude\", \n",
    "    \"accuracy\"\n",
    "]\n",
    "\n",
    "response = requests.get(download_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f'Response sucess {response.status_code}')\n",
    "    df_geo = pd.read_csv(download_url, sep=\"\\t\", header=None, names=column_names, dtype=str)\n",
    "    \n",
    "else:\n",
    "    print(f'Unsuccesful {response.status_code}')\n",
    "\n",
    "# drop duplicate rows for outward_code to avoid multiple matches at the join stage:\n",
    "df_geo_unique = df_geo.drop_duplicates(subset='outwardCode')\n",
    "# drop unwanted columns from df_geo_unique:\n",
    "df_geo_unique = df_geo_unique.drop(columns=['countryAbv', 'admin_1', 'districtName', 'onsCode', 'latitude', 'longitude', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postcode_breakdown (value): # this function finds postcodes (if contains digit assume postcode) & returns just the outward code. I will then use this outward code to reference the pgeocode DF with.\n",
    "    if isinstance(value, str) and len(value) <= 8 and any(character.isdigit() for character in value):\n",
    "        outward_code = value[:-3]\n",
    "        return outward_code\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['outwardCode'] = df['locationName'].apply(postcode_breakdown)\n",
    "df['nonPostcodeValues'] = df[\"locationName\"].where(pd.isna(df[\"outwardCode\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode = df[df['outwardCode'].notna()]\n",
    "\n",
    "\n",
    "df_non_postcode = df[df['outwardCode'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_postcode_merged = pd.merge(df_postcode, df_geo_unique, on='outwardCode', how='left')\n",
    "df_postcode_merged = df_postcode_merged.drop(columns='nonPostcodeValues_x')\n",
    "df_postcode_merged = df_postcode_merged.rename(columns={'nonPostcodeValues_y': 'nonPostcodeValues'})\n",
    "len(df_postcode_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1604"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_postcode_merged = pd.merge(df_non_postcode, df_geo_unique, on='nonPostcodeValues', how='left')\n",
    "df_non_postcode_merged =df_non_postcode_merged.drop_duplicates(subset='jobId')\n",
    "df_non_postcode_merged = df_non_postcode_merged.drop(columns='outwardCode_x')\n",
    "df_non_postcode_merged = df_non_postcode_merged.rename(columns={'outwardCode_y': 'outwardCode'})\n",
    "len(df_non_postcode_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(df_postcode_merged) + len(df_non_postcode_merged)) == df_rows:\n",
    "    df_export = pd.concat([df_postcode_merged, df_non_postcode_merged])\n",
    "else:\n",
    "    raise Exception ('data lost or gained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['jobId', 'employerId', 'employerName', 'employerProfileId',\n",
    "                'employerProfileName', 'jobTitle', 'locationName', 'minimumSalary',\n",
    "                'maximumSalary', 'currency', 'date', 'expirationDate', 'applications', 'outwardCode',\n",
    "                'countryCode', 'nonPostcodeValues', 'countryName', 'countyName', 'jobUrl', 'jobDescription']\n",
    "\n",
    "df_export = df_export[new_col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"C:/Users/Henry/Documents/Programming/Python/Python Jobs Data v1/reed_scrapper/results/reed_{keywords}_{locationName}_{df_rows}_jobs_{today}.xlsx\"\n",
    "print(f'File contains: {df_rows}')\n",
    "print(f'File location: {file_path}')\n",
    "df_export.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobs_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
